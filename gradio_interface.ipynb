{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzg9-a2jlv05",
        "outputId": "cba909fd-01e8-4549-cd21-f1dc4e3a19c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model found at: /content/drive/MyDrive/t5_summarizer_model/\n",
            " Using device: cpu\n",
            "Loading tokenizer and model...\n",
            " Model and tokenizer loaded successfully!\n",
            "Model parameters: 60,506,624\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup and Model Loading\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Define model path\n",
        "MODEL_PATH = \"/content/drive/MyDrive/t5_summarizer_model/\"\n",
        "\n",
        "# Check if model exists\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"Model not found at: {MODEL_PATH}\")\n",
        "    print(\"Please check the path and ensure your model is saved there.\")\n",
        "else:\n",
        "    print(f\" Model found at: {MODEL_PATH}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\" Using device: {device}\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "print(\"Loading tokenizer and model...\")\n",
        "try:\n",
        "    tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\" Model and tokenizer loaded successfully!\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {str(e)}\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_article(article_text):\n",
        "    \"\"\"\n",
        "    Generate summary for a given article using the fine-tuned T5 model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle empty input\n",
        "        if not article_text or article_text.strip() == \"\":\n",
        "            return \"‚ö†Ô∏è Please enter an article to summarize.\"\n",
        "\n",
        "        # Clean and prepare input\n",
        "        article_text = article_text.strip()\n",
        "\n",
        "        # Format input for T5\n",
        "        input_text = f\"summarize: {article_text}\"\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate summary\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                attention_mask=inputs.attention_mask,\n",
        "                max_length=64,\n",
        "                min_length=10,\n",
        "                num_beams=4,\n",
        "                length_penalty=2.0,\n",
        "                early_stopping=True,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        # Decode the generated summary\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Return formatted summary\n",
        "        return f\"üìÑ **Generated Summary:**\\n\\n{summary}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating summary: {str(e)}\"\n",
        "\n",
        "# Sample article for demonstration\n",
        "SAMPLE_ARTICLE = \"\"\"Breaking News: Scientists at MIT have developed a revolutionary new battery technology that could charge electric vehicles in under 5 minutes. The breakthrough involves using a novel lithium-metal composite that dramatically increases charging speed while maintaining safety standards. The research team, led by Dr. Sarah Chen, published their findings in the journal Nature Energy this week. Industry experts believe this technology could accelerate the adoption of electric vehicles worldwide by addressing one of the main consumer concerns about charging time. Major automotive manufacturers including Tesla, Ford, and BMW have already expressed interest in licensing the technology. The new batteries are expected to enter commercial production within the next 18 months, potentially revolutionizing the electric vehicle market and contributing significantly to global carbon reduction efforts.\"\"\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(),\n",
        "    title=\"News Headline Summarizer (T5 Model)\",\n",
        "    css=\"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 800px !important;\n",
        "        margin: 0 auto !important;\n",
        "    }\n",
        "    .header {\n",
        "        text-align: center;\n",
        "        margin-bottom: 30px;\n",
        "    }\n",
        "    \"\"\"\n",
        ") as interface:\n",
        "\n",
        "    # Header\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"header\">\n",
        "        <h1>üì∞ News Headline Summarizer (T5 Model)</h1>\n",
        "        <p>Powered by Fine-tuned T5 Model | Generate concise summaries from news articles</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Input section\n",
        "            article_input = gr.TextArea(\n",
        "                label=\"üìù Enter News Article\",\n",
        "                placeholder=\"Paste your news article here...\",\n",
        "                value=SAMPLE_ARTICLE,\n",
        "                lines=8,\n",
        "                max_lines=15\n",
        "            )\n",
        "\n",
        "            # Generate button\n",
        "            generate_btn = gr.Button(\n",
        "                \"üöÄ Generate Summary\",\n",
        "                variant=\"primary\",\n",
        "                scale=1\n",
        "            )\n",
        "\n",
        "            # Clear button\n",
        "            clear_btn = gr.Button(\n",
        "                \"üóëÔ∏è Clear\",\n",
        "                variant=\"secondary\",\n",
        "                scale=1\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            # Output section\n",
        "            summary_output = gr.TextArea(\n",
        "                label=\"üìÑ Generated Summary\",\n",
        "                placeholder=\"Your summary will appear here...\",\n",
        "                lines=6,\n",
        "                max_lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            # Model info\n",
        "            gr.HTML(f\"\"\"\n",
        "            <div style=\"margin-top: 20px; padding: 15px; background-color: #f0f0f0; border-radius: 8px;\">\n",
        "                <h4>üîß Model Information</h4>\n",
        "                <p><strong>Model:</strong> Fine-tuned T5 Model</p>\n",
        "                <p><strong>Device:</strong> {device}</p>\n",
        "                <p><strong>Max Input Length:</strong> 512 tokens</p>\n",
        "                <p><strong>Max Output Length:</strong> 64 tokens</p>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    generate_btn.click(\n",
        "        fn=summarize_article,\n",
        "        inputs=article_input,\n",
        "        outputs=summary_output\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: (\"\", \"\"),\n",
        "        inputs=None,\n",
        "        outputs=[article_input, summary_output]\n",
        "    )\n",
        "\n",
        "    # Examples\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [SAMPLE_ARTICLE],\n",
        "            [\"Breaking: Apple announces new iPhone with revolutionary camera technology that can capture photos in complete darkness using advanced AI algorithms.\"],\n",
        "            [\"Climate scientists warn that global temperatures are rising faster than predicted, with Arctic ice melting at an unprecedented rate according to new NASA satellite data.\"]\n",
        "        ],\n",
        "        inputs=article_input,\n",
        "        outputs=summary_output,\n",
        "        fn=summarize_article,\n",
        "        cache_examples=False\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "print(\"üöÄ Launching Gradio Interface...\")\n",
        "interface.launch(\n",
        "    share=True,\n",
        "    debug=False,\n",
        "    show_error=True,\n",
        "    server_port=7860,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Interface launched successfully!\")\n",
        "print(\"üîó You can now use the interface to summarize news articles!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "pF8GMkvTmngp",
        "outputId": "0f2f3580-4cdb-4762-88de-400642862b84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching Gradio Interface...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ceacc2e417890d8fd3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ceacc2e417890d8fd3.gradio.live\" width=\"100%\" height=\"600\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interface launched successfully!\n",
            "üîó You can now use the interface to summarize news articles!\n"
          ]
        }
      ]
    }
  ]
}